{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "#!pip install mecab-python3\n",
    "#!pip install googletrans\n",
    "from googletrans import Translator\n",
    "import MeCab\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基本形で分かち書きをする関数\n",
    "\n",
    "def analysis(text):\n",
    "    #mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    mecab.parse(\"\")\n",
    "    mecab.parseToNode(\"dummy\")\n",
    "    node = mecab.parseToNode(text)\n",
    "    word = \"\"\n",
    "    pre_feature = \"\"\n",
    "    while node:\n",
    "         # 名詞、形容詞、動詞、形容動詞であるかを判定する。\n",
    "        isUsed = \"名詞\" in node.feature\n",
    "        isUsed = \"形容詞\" in node.feature or isUsed\n",
    "        isUsed = \"動詞\" in node.feature or isUsed\n",
    "        isUsed = \"形容動詞\" in node.feature or isUsed\n",
    "         # 以下に該当する場合は除外する。（ストップワード）\n",
    "        isUsed = (not \"代名詞\" in node.feature) and isUsed\n",
    "        isUsed = (not \"助動詞\" in node.feature) and isUsed\n",
    "        isUsed = (not \"非自立\" in node.feature) and isUsed\n",
    "        isUsed = (not \"数\" in node.feature) and isUsed\n",
    "        isUsed = (not \"人名\" in node.feature) and isUsed\n",
    "        if isUsed:\n",
    "            word += \" {0}\".format(node.feature.split(\",\")[6])\n",
    "        '''\n",
    "        if isUsed:\n",
    "            if (\"名詞接続\" in pre_feature and \"名詞\" in node.feature) or (\"接尾\" in node.feature):\n",
    "            word += \"{0}\".format(node.surface)\n",
    "        else:\n",
    "        word += \" {0}\".format(node.surface)\n",
    "        #print(\"{0}{1}\".format(node.surface, node.feature))\n",
    "        '''\n",
    "        pre_feature = node.feature\n",
    "        node = node.next\n",
    "    return word[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(data_path, stopwords_path, myDoc2Vec_path):\n",
    "    ## csvの読み込み\n",
    "    df = pd.read_csv(data_path, encoding='utf-8', dtype={'Rating Score':'float'})\n",
    "    df = df.dropna(axis=0, how='all', subset=['reviews'])\n",
    "    ## lists of reviews and titles\n",
    "    reviews_list = df['reviews'].T.tolist()\n",
    "    titles_list = df['title'].T.tolist()\n",
    "    ## stopword\n",
    "    stopwords_df = pd.read_csv(stopwords_path, encoding='utf-8')\n",
    "    stopwords_list = stopwords_df.T.values.tolist()[0]\n",
    "    ## make training data\n",
    "    splitted_reviews = [analysis(reviews).split(' ') for i, reviews in enumerate(reviews_list)]\n",
    "    without_stopwords = [word for word in splitted_reviews if word not in stopwords_list]\n",
    "    without_stopwords = []\n",
    "    for i in range(len(splitted_reviews)):\n",
    "        without_stopwords.append([word for word in splitted_reviews[i] if word not in stopwords_list])\n",
    "    trainings = [TaggedDocument(words = words ,tags = [titles_list[i]]) for i, words in enumerate(without_stopwords)]\n",
    "    # save model\n",
    "    model = Doc2Vec(documents=trainings, dm=1, vector_size=200, alpha=0.025, min_count=1)\n",
    "    model.save(myDoc2Vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords_path='data/stopwords.csv'):\n",
    "    splitted_reviews = analysis(text).split(' ')\n",
    "    stopwords = pd.read_csv(stopwords_path, encoding='utf-8').T.values.tolist()[0]\n",
    "    return [word for word in splitted_reviews if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movies(text, model_path='doc2vec523_neologd.model'):\n",
    "    model = Doc2Vec.load(model_path)\n",
    "    vec = model.infer_vector(preprocess(text))\n",
    "    return model.docvecs.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(japanese_list):\n",
    "    translator = Translator()\n",
    "    predict_movies(\"心温まる\")\n",
    "    translations = [word.text for word in translator.translate(japanese_list)]\n",
    "    return [word.text for word in translator.translate(translations, dest='ja')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('data/523_reviews_df.csv', 'data/stopwords.csv', 'doc2vec523_neologd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ゴースト／ニューヨークの幻', 0.7543773651123047),\n",
       " ('くちびるに歌を', 0.7306462526321411),\n",
       " ('あやしい彼女', 0.7249703407287598),\n",
       " ('ジョゼと虎と魚たち', 0.7220292091369629),\n",
       " ('リトル・ミス・サンシャイン', 0.7220073938369751),\n",
       " ('ヘアスプレー', 0.7196100354194641),\n",
       " ('マイ・フレンド・フォーエバー', 0.7193808555603027),\n",
       " ('ホリデイ', 0.7148844599723816),\n",
       " ('ラブ・アクチュアリー', 0.7142664194107056),\n",
       " ('ワン・デイ\\u300023年のラブストーリー', 0.7138307094573975)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_movies(\"泣いちゃうよ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('doc2vec.model')\n",
    "model.docvecs.most_similar('アナと雪の女王2', topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('doc2vec_neologd.model')\n",
    "model.docvecs.most_similar('アナと雪の女王2', topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_movies(\"泣ける\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(\"正直1作目のアナ雪は好きではなかった。もちろん音楽と映像は素晴らしかったしキャラクターは魅力的だった。\"))\n",
    "print(preprocess_translation(preprocess(\"正直1作目のアナ雪は好きではなかった。もちろん音楽と映像は素晴らしかったしキャラクターは魅力的だった。\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_translation(preprocess(\"犬が庭でとても寒そうにしています\")))\n",
    "print(preprocess_translation(preprocess(\"庭で犬がすごく寒そうにしている\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess(\"犬が庭でとても寒そうにしています\"))\n",
    "print(preprocess(\"庭で犬がすごく寒そうにしている\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
